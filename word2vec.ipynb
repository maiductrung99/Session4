{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import  isfile\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import tensorflow.compat.v1 as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 500  #cua 1 docunment\n",
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        self._sentence_lengths = []\n",
    "        for _, line in enumerate(d_lines):\n",
    "          if len(line) > 1:\n",
    "            features = line.split(\"<fff>\")\n",
    "            label, sentence_length = int(features[0]), int(features[2])\n",
    "            tokens = features[3].split()\n",
    "            vector = [int(token) for token in tokens]\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "            self._sentence_lengths.append(sentence_length)\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
    "        \n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "\n",
    "        if end + self._batch_size > len(self._data):\n",
    "            end = len(self._data)\n",
    "            start = end - self._batch_size\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2020)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels, self._sentence_lengths= self._data[indices], self._labels[indices], self._sentence_lengths[indices]\n",
    "\n",
    "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_and_vocab():\n",
    "    def collect_data_from(parent_path, newsgroup_list, word_count= None):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "            dir_path = parent_path + '/' + newsgroup + '/'\n",
    "            \n",
    "            files= [(filename, dir_path + filename)\n",
    "                       for filename in listdir(dir_path) if\n",
    "                       isfile(dir_path + filename)]\n",
    "            files.sort\n",
    "            label = group_id\n",
    "            print('Processing: {}-{}'.format(group_id, newsgroup))\n",
    "            \n",
    "            for filename, filepath in files:\n",
    "                with open(filepath) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split('\\W+', text)\n",
    "                    if word_count is not None:\n",
    "                        for word in words:\n",
    "                            word_count[word] +=1\n",
    "            content = ' '.join(words)\n",
    "            assert len(content.splitlines()) == 1\n",
    "            data.append(str(label) + '<fff>' + filename + '<fff>' + coneent)\n",
    "        return data\n",
    "    word_count= defaultdict(int)\n",
    "    \n",
    "    #path of data\n",
    "    path='./datasets/20news-by-date/'\n",
    "    parts = [path + dir_name + '/' for dir_name in listdir(path) \n",
    "                if not isfile(path+ dir_name)]\n",
    "    \n",
    "    #take the path of train and test forder\n",
    "    train_path, test_path = (parts[0], parts[1]) \\\n",
    "        if 'train' in parts[0] else (part[1],part[0])\n",
    "    \n",
    "    newsgroup_list= [newsgroup for newsgroup in listdir(train_path)]\n",
    "    newsgroup_list.sort()\n",
    "    \n",
    "    #set train data\n",
    "    train_data= collect_data_from(\n",
    "        parent_path = train_path,\n",
    "        newsgroup_list = newsgroup_list,\n",
    "        word_count = word_count\n",
    "    )\n",
    "    \n",
    "    #set up and sort vocabulary\n",
    "    vocab = [word for word, freq in\n",
    "                 zip(word_count.keys(), word_count.values()) if freq > 10]\n",
    "    vocab.sort()\n",
    "             \n",
    "    #write vocab on file         \n",
    "    with open('./datasets/w2v/vocab-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "    \n",
    "    #set up test data         \n",
    "    test_data = collect_data_from(\n",
    "        parent_path = test_path,\n",
    "        newsgroup_list = newsgroup_list\n",
    "    )\n",
    "    \n",
    "    #write train and test data on file\n",
    "    with open('./datasets/w2v/20news-train-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open('./datasets/w2v/20news-test-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path, vocab_path):\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_ID + 2)\n",
    "                        for word_ID, word in enumerate(f.read().splitlines())])\n",
    "    with open(data_path) as f:\n",
    "        documents= [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
    "                       for line in f.read().splitlines()]\n",
    "    encoded_data = []\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:MAX_DOC_LENGTH]\n",
    "        sentence_length = len(words)\n",
    "    \n",
    "    encoded_text = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoded_text.append(str(vocab[word]))\n",
    "        else:\n",
    "            encoded_text.append(str(unknown_ID))\n",
    "    if len(words) < MAX_DOC_LENGTH:\n",
    "        num_padding = MAX_DOC_LENGTH - len(words)\n",
    "        for _ in range(num_padding):\n",
    "            encoded_text.append(str(padding_ID))\n",
    "    encoded_data.appen(str(label)+ '<fff>' + str(doc_id) + '<fff>' +\n",
    "                       str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
    "    \n",
    "    dir_name= '/'.join(data_path.split('/')[:-1])\n",
    "    file_name= '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
    "    with open(dir_name + '/' + file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._lstm_size = lstm_size\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
    "        self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "        self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "        \n",
    "    def embedding_layer(self, indices):\n",
    "        pretrained_vectors= []\n",
    "        pretrained_vectors.append(np.zeros(self._embedding_size))\n",
    "        np.random.seed(2018)\n",
    "        for _ in range(self._vocab_size + 1):\n",
    "            pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
    "        \n",
    "        pretrained_vectors = np.array(pretrained_vectors)\n",
    "        \n",
    "        self._embedding_matrix = tf.get_variable(\n",
    "            name= 'embedding',\n",
    "            shape= (self._vocab_size +2, self._embedding_size),\n",
    "            initializer= tf.constant_initializer(pretrained_vectors)\n",
    "        )\n",
    "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
    "    \n",
    "    def LSTM_layer(self, embeddings):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
    "        zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
    "        initial_state= tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
    "        \n",
    "        lstm_inputs = tf.unstack(\n",
    "            tf.transpose(embeddings, perm=[1,0,2])\n",
    "        )\n",
    "        \n",
    "        lstm_outputs, last_state = tf.nn.static_rnn(\n",
    "            cell= lstm_cell,\n",
    "            inputs= lstm_inputs,\n",
    "            initial_state= initial_state,\n",
    "            sequence_length= self._sentence_lengths\n",
    "        )\n",
    "        \n",
    "        lstm_outputs = tf.unstack(\n",
    "            tf.transpose(lstm_outputs, perm=[1,0,2]))\n",
    "        lstm_outputs = tf.concat(\n",
    "            lstm_outputs,\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        mask=tf.sequence_mask(\n",
    "            lengths= self._sentence_lengths,\n",
    "            maxlen= MAX_DOC_LENGTH,\n",
    "            dtype= tf.float32\n",
    "        )\n",
    "        mask= tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
    "        mask= tf.expand_dims(mask, -1)\n",
    "        lstm_outputs = mask * lstm_outputs\n",
    "        lstm_outputs_split= tf.split(lstm_outputs, num_or_size_splits = self._batch_size)\n",
    "        lstm_outputs_sum= tf.reduce_sum(lstm_outputs_split, axis=1)\n",
    "        lstm_outputs_average= lstm_outputs_sum / tf.expand_dims(\n",
    "            tf.cast(self._sentence_lengths, tf.float32), -1)\n",
    "        return lstm_outputs_average\n",
    "    def build_graph(self):\n",
    "        embeddings = self.embedding_layer(self._data)\n",
    "        lstm_outputs= self.LSTM_layer(embeddings)\n",
    "        \n",
    "        weights = tf.get_variable(\n",
    "            name= 'final_layer_weights',\n",
    "            shape= (self._lstm_size, NUM_CLASSES),\n",
    "            initializer = tf.random_normal_initializer(seed=2018),\n",
    "        )\n",
    "        biases = tf.get_variable(\n",
    "            name = 'final_layer_biases',\n",
    "            shape= (NUM_CLASSES),\n",
    "            initializer= tf.random_normal_initializer(seed=2018),\n",
    "        )\n",
    "        \n",
    "        logits= tf.matmul(lstm_outputs, weights) + biases\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(\n",
    "            indices= self._labels,\n",
    "            depth= NUM_CLASSES,\n",
    "            dtype= tf.float32\n",
    "        )\n",
    "        \n",
    "        loss= tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels_one_hot,\n",
    "            logits=logits\n",
    "        )\n",
    "        loss= tf.reduce_mean(loss)\n",
    "        \n",
    "        probs=tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels= tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op= tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_RNN():\n",
    "    with open('./datasets/w2v/vocab-raw.txt') as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "    tf.set_random_seed(2020)\n",
    "    rnn = RNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_size=300,\n",
    "        lstm_size=50,\n",
    "        batch_size=50\n",
    "    )\n",
    "    predicted_labels, loss = rnn.build_graph()\n",
    "    train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        train_data_reader = DataReader(\n",
    "            data_path='./datasets/w2v/20news-train-encoded.txt',\n",
    "            batch_size=50\n",
    "        )\n",
    "        test_data_reader = DataReader(\n",
    "            data_path='./datasets/w2v/20news-test-encoded.txt',\n",
    "            batch_size=50\n",
    "        )\n",
    "        step = 0\n",
    "        MAX_STEP = 1000 ** 2\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        while step < MAX_STEP:\n",
    "            next_train_batch = train_data_reader.next_batch()\n",
    "            train_data, train_labels, train_sentence_lengths = next_train_batch\n",
    "            plabels_eval, loss_eval, _ = sess.run(\n",
    "                [predicted_labels, loss, train_op],\n",
    "                feed_dict={\n",
    "                    rnn._data: train_data,\n",
    "                    rnn._labels: train_labels,\n",
    "                    rnn._sentence_lengths: train_sentence_lengths\n",
    "                }\n",
    "            )\n",
    "            step += 1\n",
    "            if step % 20 == 0:\n",
    "                print('loss:', loss_eval)\n",
    "\n",
    "            if train_data_reader._batch_id == 0:\n",
    "                num_true_preds = 0\n",
    "                while True:\n",
    "                    next_test_batch = test_data_reader.next_batch()\n",
    "                    test_data, test_labels, test_sentence_lengths = next_test_batch\n",
    "\n",
    "                    test_plabels_eval = sess.run(\n",
    "                        predicted_labels,\n",
    "                        feed_dict={\n",
    "                            rnn._data: test_data,\n",
    "                            rnn._labels: test_labels,\n",
    "                            rnn._sentence_lengths: test_sentence_lengths\n",
    "                        }\n",
    "                    )\n",
    "                    matches = np.equal(test_plabels_eval, test_labels)\n",
    "                    num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "                    if test_data_reader._batch_id == 0:\n",
    "                        break\n",
    "\n",
    "                print('Epoch:', train_data_reader._num_epoch)\n",
    "                print('Accuracy on test data:', num_true_preds * 100. / len(test_data_reader._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-6-565e24bcc630>:29: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-565e24bcc630>:37: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-6-565e24bcc630>:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "loss: 0.00309619\n",
      "loss: 0.15606037\n",
      "loss: 4.370958\n",
      "loss: 0.37584746\n",
      "loss: 0.97326803\n",
      "loss: 4.8714995\n",
      "loss: 1.9352441\n",
      "loss: 2.1403348\n",
      "loss: 2.2674253\n",
      "loss: 4.4915414\n",
      "loss: 5.79694\n",
      "Epoch: 1\n",
      "Accuracy on test data: 7.141908934023629\n",
      "loss: 2.9055252\n",
      "loss: 2.410326\n",
      "loss: 2.4434178\n",
      "loss: 2.0913978\n",
      "loss: 1.9201926\n",
      "loss: 1.800817\n",
      "loss: 1.9295263\n",
      "loss: 1.6944369\n",
      "loss: 1.5658292\n",
      "loss: 1.3457245\n",
      "loss: 1.4176983\n",
      "Epoch: 2\n",
      "Accuracy on test data: 70.75534315677685\n",
      "loss: 1.2645135\n",
      "loss: 0.91595095\n",
      "loss: 0.81038904\n",
      "loss: 0.79186666\n",
      "loss: 0.8878096\n",
      "loss: 0.77714837\n",
      "loss: 0.80821973\n",
      "loss: 0.5377564\n",
      "loss: 0.46624267\n",
      "loss: 0.7231693\n",
      "loss: 0.5094177\n",
      "Epoch: 3\n",
      "Accuracy on test data: 75.7201646090535\n",
      "loss: 0.23869461\n",
      "loss: 0.24323636\n",
      "loss: 0.26632765\n",
      "loss: 0.23658267\n",
      "loss: 0.19179127\n",
      "loss: 0.25477523\n",
      "loss: 0.24150118\n",
      "loss: 0.273317\n",
      "loss: 0.26864854\n",
      "loss: 0.18621297\n",
      "loss: 0.23876774\n",
      "loss: 0.3556212\n",
      "Epoch: 4\n",
      "Accuracy on test data: 77.53882915173237\n",
      "loss: 0.035588633\n",
      "loss: 0.066635944\n",
      "loss: 0.060122196\n",
      "loss: 0.05402245\n",
      "loss: 0.0360278\n",
      "loss: 0.08964885\n",
      "loss: 0.032705072\n",
      "loss: 0.045377124\n",
      "loss: 0.059682198\n",
      "loss: 0.026014183\n",
      "loss: 0.0652177\n",
      "Epoch: 5\n",
      "Accuracy on test data: 76.88835789194212\n",
      "loss: 0.013983076\n",
      "loss: 0.0085958475\n",
      "loss: 0.017209956\n",
      "loss: 0.012661386\n",
      "loss: 0.010014522\n",
      "loss: 0.010385811\n",
      "loss: 0.025494082\n",
      "loss: 0.00832384\n",
      "loss: 0.006594967\n",
      "loss: 0.016126385\n",
      "loss: 0.015277359\n",
      "Epoch: 6\n",
      "Accuracy on test data: 77.11403159431833\n",
      "loss: 0.0053791315\n",
      "loss: 0.005276302\n",
      "loss: 0.0044392115\n",
      "loss: 0.007142155\n",
      "loss: 0.0032694412\n",
      "loss: 0.0034955323\n",
      "loss: 0.004422056\n",
      "loss: 0.004107993\n",
      "loss: 0.0042950306\n",
      "loss: 0.0064030504\n",
      "loss: 0.005500178\n",
      "loss: 0.024078554\n",
      "Epoch: 7\n",
      "Accuracy on test data: 77.44590468604805\n",
      "loss: 0.0039662784\n",
      "loss: 0.0020037845\n",
      "loss: 0.0025705465\n",
      "loss: 0.011755683\n",
      "loss: 0.002887784\n",
      "loss: 0.0019184953\n",
      "loss: 0.0030653058\n",
      "loss: 0.0018654938\n",
      "loss: 0.002520446\n",
      "loss: 0.0020699995\n",
      "loss: 0.0025301222\n",
      "Epoch: 8\n",
      "Accuracy on test data: 77.12730651798752\n",
      "loss: 0.0014645006\n",
      "loss: 0.0015877719\n",
      "loss: 0.0017499307\n",
      "loss: 0.0015697377\n",
      "loss: 0.03434742\n",
      "loss: 0.0012779043\n",
      "loss: 0.0019945176\n",
      "loss: 0.00094958866\n",
      "loss: 0.0013657376\n",
      "loss: 0.0010234497\n",
      "loss: 0.033199586\n",
      "Epoch: 9\n",
      "Accuracy on test data: 77.512279304394\n",
      "loss: 0.00095271243\n",
      "loss: 0.0012345411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-33202bce0288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_and_evaluate_RNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-4bc6cc263268>\u001b[0m in \u001b[0;36mtrain_and_evaluate_RNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mnext_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_sentence_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_train_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             plabels_eval, loss_eval, _ = sess.run(\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 feed_dict={\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    train_and_evaluate_RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
